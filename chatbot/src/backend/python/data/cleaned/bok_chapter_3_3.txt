--- Page 1 --- Page 1 of 18 Section 3 Internal Governance Dealing with internal governance structures & measures Chapter
3.3 Managing & Addressing Risks  Understanding concepts of risks related to deploying AI/ML systems.  Processes to design and implement AI/ML systems to better manage risks. Authors: Tomithy Too & Devesh Narayanan
1. Introduction
1.1 Understanding Risk
1.2 Dealing with Risk
1.3 Risks & AI: An example
2. Risk Management
2.1 Illusion of Control
2.2 Vulnerabilities
2.3 AI Complexities
3. AI Implementation
3.1 Flow & Mitigation
3.2 Stress Testing
3.3 Iterative Design & Testing
3.3 Testing Environments
3.4 Monitoring
4. Checklist
5. References
1. Introduction AI systems are already being adopted in many organisations, and for a good reason: AI systems promise unprecedented benefits in a wide variety of use-cases. At the same time, AI comes with myriad risks; some might aggravate existing risks to organisations, and some create new ones. AI is a double-edged sword, but "both sides of the blade are far sharper, and neither is well understood"[1]. As AI systems begin to be widely deployed, the frequency and seriousness of such risks may increase. The impact of such risks – monetary, reputational, societal, or other types – could vary widely; from massive data losses and reputational damages to minor inconveniences like a reboot. The likelihood of such risks is equally varied – from being very improbable to very likely. Moreover, even when an AI system appears to be functioning normally, risks that negatively impact the system and its stakeholders could potentially occur at any moment. --- Page 2 --- Page 2 of 18 Anticipating and dealing with risk is what all organisations need to do as part of their business continuity practices. This chapter focuses on the risks surrounding AI/ML systems, as well as related business, software and general process risks, to ensure that all potential weak links in the risk management process are addressed.
1.
1. Understanding Risk While the risks associated with AI systems are often unprecedented, general strategies and best practices for risk management can provide a useful starting point for managing these new risks. Similar to any other complex system, risk management for AI requires organisations to have a deep understanding of how systems function, how they are expected to behave, and how they could add value or cause damage to their stakeholders. This is crucial for anticipating and mitigating risks. Organisations need to adopt a framework that allows for all relevant risks to be anticipated and strategised, but not become paranoid with unwarranted suspicion. Risk managers practising "adversarial thinking" should carefully probe and, where possible, reverse- engineer the system to understand its architecture, nuances, edge cases and hidden behaviours. In so doing, they build a deeper understanding of the system's vulnerabilities, including those that may not be immediately apparent from typical usage or customisation. It is a process of continuous refinement: even as risks are identified and dealt with, practitioners of "adversarial thinking" should continually be on the lookout for new and emergent risks, and ready to deal with them as they occur. However, humans by nature prefer to avoid risk and tend to perceive high impact but low probability risks (such as aircraft crashes) as more significant, more urgent threats. Such paranoia might over-price risks, increase governance and other precautionary measures. It may also hinder a company's ability to implement projects or adopt new technologies.
1.
2. Dealing with Risk Mitigating risk is not just trying to eliminate it, but also attempting to reducing its probability or impact, and being able to recover from the damage. Acceptable risk management practices are, therefore, continuous efforts. They require consistent visibility into and diligent monitoring of systems and their interactions. Good risk management, thus, is akin to a sound radar system that provides both lead time and contextual information to reduce or eliminate risks before they escalate. The following questions are a high-level overview of general risk management strategies: a. Prioritising  What risks does the organisation care about most?  What are the most valuable aspects of the system? (Such as business functioning, reputation, data, user safety, user well-being, and others).  Which stakeholders are the organisation trying to protect?  What is the worst possible impact of each risk to the organisation's valuable assets?  What is the average/expected impact of each risk?  Which risks are most likely to occur? --- Page 3 --- Page 3 of 18 b. Mitigating  How can the organisation know about impending risks?  What resources are available to deal with an identified risk before its impact is realised? c. Pre-empting  How does the team prepare for future risks?  What are the audit controls, process design controls and technical (automated) controls to address future risks?
1.
3. Risks & AI: An Example The consumer banking division of Bank X plans to deploy an AI-based credit-scoring system that can approve consumer loans up to a certain amount within a few minutes of receiving a completed online application. Here is some contextual information about this deployment:  Vendor A claims that its AI-based credit scoring system is accurate, faster, and cost- effective in performing Know Your Customer (KYC) checks and determining default rates as compared to an experienced loan officer.  Bank X's officials cannot edit the core algorithm or implicit hyperparameters of the AI system, or change the data that is used to pre-train the model. However, some hyperparameters and diagnostics are made available to the team via API and configuration pages to set up the model.  Based on the business objectives and operating requirements, the Bank X, in consultation with the vendor, decides on confidence levels, trade-offs between the accuracy of different outcomes, and the inputs of the training, testing, validation and production data for the system.  Vendor A claims that the system has been deployed by various fintech companies, and is currently on trial with other banks. The algorithm can learn from the datasets of various companies to improve the algorithm's accuracy. In all deployments elsewhere, the algorithm has outperformed the loan officer's prediction in assessing credit risk. Which factors should Bank X consider before deploying this system?  Understand the system: Have internal teams rigorously test the system – as well as the environment in which it operates. Identify the stakeholders, assets and potential vulnerabilities.  Identify Risks: Brainstorm with stakeholders to identify the risks. This can be done via interviews, "premortem" analysis, comparison with precedence, and other analogous systems.  Prioritise Risks: Estimate the probability of each risk, and identify the possible primary and secondary impacts of the risks, in collaboration with the leadership and operational teams.  Mitigating Solutions: Brainstorm ways to reduce the likelihood/impact of each critical risk. Calculate costs for each mitigation. Prioritise feasible solutions that address multiple risks.  Monitor & Reassess: Validate the effectiveness of risk mitigation as an on-going process until that risk no longer poses a threat. Monitor the system for any anomalous activity. Periodically reassess for new risks that might emerge. --- Page 4 --- Page 4 of 18 Bank X does its risk-assessment exercise by identifying stakeholders in deploying the system.  Critical stakeholders within the company: Loan officers, commercial lending teams, IT and cyber teams, risk and audit teams, the head of consumer banking and other senior management.  Key stakeholders outside the company: Vendors, government regulators, customers, and others.  Critical assets and concerns for each of these stakeholders are then identified.  Bank X embarks on a technical and deployment deep-dive in consultation with relevant vendors.  The diagram provides a simplified deployment architecture for this system.  It shows how various stakeholders interact with each other to anticipate risks: Figure 1: Simplified deployment credit-scoring AI architecture (Source: Chapter authors) The following non-exhaustive list of risks and vulnerabilities have been identified:  Vendor A has login access to the system for the trial.  Vendor A has access to the test data, which is anonymised – but not tokenised.  The system is dependent on a legacy backend that might have scalability issues.  Customers may apply for loans either using the online system or in-person.  Current processes do not have adequate checks to rectify duplicate applications.  The system will be in the same production environment as other bank web systems.  While that aids interoperability, it may also introduce instabilities.  The management must have the authentication to override loans decision manually.  Audit oversight might be insufficient for this new process.  The KYC process of clients relies on third-party APIs, which may fail.  Five known minor bugs exist for the AI system, as declared by Vendor A.  Loans made by credit scoring are not yet subject to insurance. <|image_start|>bok_chapter_3_3_page4_img
1.jpeg<|image_end|> --- Page 5 --- Page 5 of 18  Customers may feel that the AI-based credit scoring system was unfair towards them.  The explanations provided might not be sufficient to counter this perceived unfairness.  Customers may try to game the system with impersonation. The risks can be arranged in terms of their likelihood and impact. An overview of the risks would help the organisation to strategize, prioritise and possibly mitigate critical risks, as depicted below: Figure 2: Visual mapping of risk (Source: Chapter authors)
2. Risk Management AI systems can be used to automate decisions or actions instead of humans. In the example above, Bank X is entrusting the credit scoring AI to make lending decisions that would otherwise be made by its loan officers. In exceptional cases (incomplete information submitted, applicant's credit history being suspect, among others), the system will defer judgement to loan officers. It is crucial for any organisation planning to implement AI systems to define which decisions can be taken by the AI, and which cannot. The criteria listed below may be of help: a. Scalability Scalability, cost-efficiency, performance – these are typical considerations that motivate the adoption of AI systems. These systems must bring enough value to the company to justify its deployment. b. Severity of Harm Some decisions have the potential to create massive and sustained harm to affected stakeholders (such as in medical diagnoses, predicting recidivism, and others.). In such cases, if deploying the system is still deemed worthwhile, companies may consider having AI systems augment – rather than replace – a human decision-maker. <|image_start|>bok_chapter_3_3_page5_img
1.jpeg<|image_end|> --- Page 6 --- Page 6 of 18 c. Repeatability Routine decisions can be automated using AI. It is not feasible to automate processes that only happen rarely. Exceptions are either events not in the usual process flow, or events that AI systems are not capable of handling. Such events should be flagged for humans decision-makers. d. Externalities AI systems do not exist in a vacuum; they often interface with other technical, social or economic systems in the real world. For an AI system entrusted with making critical decisions, any malfunction could lead to other malfunctions downstream, or in interconnected systems. o Example: AI is being deployed in military applications[2]. There are some advantages: replacing human soldiers with AI robots, creating autonomous weapons, reducing the risk of harm, etc. However, any malfunction in an AI system could lead to severe negative consequences. It would be a gross mistake for any risk assessment of such systems to exclude associated political, social and economic risks. External risks are not always apparent. That's because AI systems are connected to several other technical, social and economic systems. Holistic risk assessments ought to take into account all these external risks as well.
2.
1. Illusion of Control There is a tendency for AI development teams to underestimate the complexity of AI systems and overestimate the extent to which they can control them[3]. When the datasets are small, and complexity seems low – or if the AI system worked in predictable ways in the past, or if teams assume that AI systems operate in a deterministic, linear fashion – it is easy for the development teams to imagine that they are in control. However, this is often not the case. o Example: In the previous example, Bank X's staff may think that the credit scoring AI may always act like to a loan officer. That is true when loan applications are within specifications. The system may trigger an exception when an application deviates from the norm. However, if the team tests the algorithm under a narrow range of conditions – without triggering these exceptions – they may be lulled into a false sense of competency. Several other aspects of the AI system might be beyond the team's control. Consider these possibilities:  Vendor X may change some parameters or code during updates.  Vendor X may not keep the bank informed about code updates or changes.  Some code changes may not be apparent in the behaviour of the system.  Bank X's teams may not be aware of the new risks being introduced.  Changes in the real world, such as a credit crunch or sudden layoffs, may occur.  Changes in the real world may invalidate the predictive capability of the AI.  The AI system may be storing personally identifiable information (PII) of applicants.  Storing or viewing PII might violate Bank X's privacy policies.
2.
2. Vulnerabilities Organisations need to understand how much of the system and its data is visible and verifiable to their teams. They should not assume that AI systems will behave well – even if they appeared to do so in the testing phase. Being continually sceptical is a sound practice because it forces the organisation to run tests to discover vulnerabilities regularly. Organisations should also design and implement processes and criteria to adapt quickly to remove or replace systems in the case of failure. --- Page 7 --- Page 7 of 18 As with most processes, AI systems are only as strong as their weakest link. Each connection presents a potential vulnerability. Some of these may emerge from the design and program of the AI system itself; for instance, having incomplete or erroneous data, exception loops, implementation errors, and others. Failure to address critical vulnerabilities could render even the best intentions irrelevant and taint a project that could have otherwise brought immense value. Vulnerabilities may be of several types. Equally varied are the attacks that exploit them. Organisations should pay special attention to cyberattacks, failure of IT subsystems that the AI system depends on, human tampering, unauthorised access, and failures of external databases or transaction systems. o Example: In March 2017, Facebook launched an ambitious project to anticipate and prevent suicide with AI[4]. Following a string of suicides that were live-streamed on the platform, the effort to use AI sought to address a severe problem proactively. The algorithm touched nearly every post on Facebook, rating each piece of content on a scale from zero to one, with one expressing the highest likelihood of "imminent harm," a Facebook representative reported. o In September 2018, Facebook revealed that a large-scale data breach had exposed the profiles of about 30 million people. For 400,000 of those, posts and photos were left open. Facebook did not comment on whether data from its suicide prevention algorithm had been breached. One strategy to identify critical vulnerabilities is "premortem" analysis in which relevant stakeholders are asked to imagine that the AI system has failed and its consequences are already in effect. Stakeholders are then asked to explain the probable causes of the failure. A rigorous premortem analysis allows dissenters to have a safe space to voice their concerns before deployment, to help list out potential points of failure. a. Adversarial Attacks AI systems are constructed using complex algorithms and datasets and are vulnerable to "adversarial attacks". Malicious actors deploy an "adversary" neural network to create data that is specifically designed to fool the target AI system into making the wrong decision. o Example: Handwriting recognition algorithms may sometime confuse the numbers "2" and "7". An adversarial attack will exploit this. Similarly, an image classifier algorithm may be fooled by hidden noise. That is imperceptible to humans, but it is how AI systems process and analyse images[6]. A carefully-designed adversarial attack can exploit this, with the algorithm confusing a panda for a gibbon[5]. b. Data Poisoning Another related attack is "data poisoning". The integrity of the input data is compromised, either due to human error or by a hacker. o Example: In Bank X's algorithm, one critical criterion for deciding the loan application may be the applicant's income level. A hacker may lower the income levels of a set of applicants in the training dataset, thereby training the AI model that a lower income threshold is viable. The same effect may be observed if a large number of applicants had previously declared their income wrongly. In both cases, the integrity of the training dataset may be deemed to be "poisoned". There is a growing body of literature on the various types of adversarial attacks that may pose risks to AI systems, and insights on how to defend against them[7]. Organisations should ensure their teams are aware and updated on both the potential vulnerabilities and the mitigation measures. --- Page 8 --- Page 8 of 18
2.3 AI Complexities All AI systems are complex and dynamic. They can behave unexpectedly. This complexity chiefly arises out of two properties: non-determinism and generative complexity. a. Non-Determinism Non-deterministic processes occur when the same output is not guaranteed even when the same inputs are used[8]. In many AI systems, the output is defined as a probabilistic function of the input, and one or more random variables are used to train the model. Non-deterministic complexities are aggravated when the training or input data is incomplete, or if multiple AI systems are connected for analysis. Their non-deterministic nature makes it hard to troubleshoot or test for errors. Examples of non-deterministic algorithms: neural networks[9], simulated annealing[10], genetic algorithms[11], and Markov processes[12]. o Example: Bank X's credit scoring AI may give a different loan approval amount or rating even if identical profiles for loan applicants were fed into the system. The same training set may produce a different final model because of the random shuffling and sampling of the training data. b. Generative AI systems may learn as they are exposed to more data (or maybe even edit their own codebase for better fit or performance, in the future). That could induce several unexpected results that are difficult to detect. The goal of risk management of complex AI systems is to ensure that these systems are well-behaved. Well-behaved refers to the system displaying the following properties:  It performs as documented for any input dataset – synthetic or actual – in tests and production.  The output and actions of the system are restricted to a predefined set of outcomes.  No new outcomes or secondary effects result as a direct consequence of the AI system's outputs.  Any modifications/updates to the system are tracked; the system is re-tested after any change.
3. AI Implementation McKinsey offers a primer to understand the risks associated with AI from a process and implementation perspective[13]. These are denoted in blue in the table below. This section discusses these insights – augmenting them with a growing global effort to standardise ethical design in AI[14][15][16]. --- Page 9 --- Page 9 of 18 Conceptual- isation & Planning Data Collection & Preparation Model Develop & Implement Model Deploymt & Online Learn User Interaction Other Issues Potentially unethical use cases Incomplete or inaccurate data Non- representati ve data Implementat ion errors Slow detection of/response to performance issues Open sourcing the model Insufficient learning feedback loop Insecure data Biased or discriminato ry model Poor model performance Failure at usability or explainabilit y Open- sourcing of data Non- regulatory compliance Implementat ion errors Cybersecurit y threats Malicious user intent Model invalidation due to change of environment Non- conformance to privacy consent and use of data Insufficient training and skills Model operated by people who are not data- science trained or given enough context Model may not give user enough context, or outputs cannot be explained Offline model that cannot be updated Model instability or performance degradation Failure or changes of 3rd party tool Errors introduced by Hand tuning Base model may contain discriminati on Figure 3: Impact & costs to recover (Source: Extended from McKinsey & Company)
3.
1. Flow & Mitigation The list below is an overview of the main risks that need to be considered when developing or deploying AI systems. Not all risks may be relevant to your organisation or your AI model. Knowing what to check, when and how is crucial for effective risk management. --- Page 10 --- Page 10 of 18 a. PII Personally Identifiable Information (PII) is highly sensitive and must be treated with the utmost confidentiality. Specific interactions with third-party vendors, data partners, public platforms, and others may risk non-conformance to privacy directives. Risks could also be introduced when project teams are uncertain about the agreed or intended uses of data. KYC data may inadvertently be used for marketing, which may pose legal challenges under Singapore's PDPA[17]. Some mitigations:  Have clearly-defined intentions for the use of data, especially PII data.  Educate project teams and vendors on the intended use of PII data.  Carry out adequate anonymisation of PII for use in data analytics. b. Hand Tuning This occurs when data points are manually selected or added for better performance. While applying human judgement may improve the algorithm's performance within specific contexts, these actions may introduce unwanted bias, untraceable errors and various other mistakes. Some mitigations:  Track and log all user actions to allow for audit and change-tracking.  Modifications to data/algorithm/model should be discussed/debated.  Test tuned models for anomalous data, and bias introduced by humans. c. Bias from Base Models Building on the work of pre-trained base models to bootstrap the learning and performance of AI systems is called transfer learning. This saves training time, resources and gathering of data. However, base models may introduce new biases, since these models come with pre-tuned weights, or as a black box. Some mitigations:  Get detailed explanations for base models on their context and limitations.  Test the models on your dataset to check for pre and post-trained bias.  If necessary, re-engineer the base mode and train it from scratch. d. Training Data science and statistics are not standard skillsets in many organisations. This may result in incorrect implementation or testing. A model used out of context, without being adequately tested for errors, could be hazardous and its outcomes suspect. Some mitigations:  Train developers/implementors/operators on statistics and data science.  Get the team to understand the context and the weakness of the system.  Get external consultants to train and deploy the system and educate users. e. Third-Parties Unless custom-developed for your organisation, the codebase of third party tools or systems is not usually available to you. With the advent of agile software development, the pace of code release is also more rapid; minor versions may be released as frequently as every few hours. Any customisation or automation could make the system brittle and introduce new risks. Some mitigations:  Negotiate appropriate SLAs for the tools or systems.  Negotiate for support for fixing the systems in case of failure.  Negotiate for indemnity if the software fails to achieve specific clauses in the SLAs.  Ask your vendor/s to share their development roadmap.  Get them to coordinate with the ops team before any changes are made. --- Page 11 --- Page 11 of 18 f. Hackers Hackers may pose a wide variety of threats to AI systems[18]. Internal hackers may have access to the system components and may be able to alter configurations, data, hide access logs, etc. External hackers may attempt to exploit vulnerabilities in various ways, as the cost of failure to them is low. Mistakes from internal users may compound the problem; the outcome of such mistakes might be indistinguishable from those of malicious actors. Internal users may also misreport/misrepresent the performance of the systems to clear administrative requirements. Some mitigations:  Consider setting up "red teams" (IT teams charged with hacking to find vulnerabilities).  Fix vulnerabilities that the "red teams" unearth.  Ensure rigorous IT/data governance and cybersecurity controls.  Ensure organisation-wide adherence to cybersecurity protocols and hygiene.  Implement systems to detect and mitigate insider threats.  Separate roles using access control (engineers may change code, but not user data; data scientists may not use systems outside the particular models they are working on). g. Updates AI systems may be sometimes be embedded in devices, with low or no connectivity. They cannot then be updated and can't report any errors. Such use-cases are becoming increasingly available, with the proliferation of IoT devices. Some mitigations:  Empower end-users to perform checks on the system to ensure they are functioning as intended.  Design the system specifically to function without access to external communications. h. Open Source Organisations may potentially benefit from opening their algorithms or models to the open-source community. However, malicious developers might be able to identify potential vulnerabilities with the system with greater clarity and exploit these in an attack. Unscrupulous people may repurpose the open-source model or algorithm for criminal or unethical purposes (such as using AI to generate deep fakes). The model may be reverse-engineered to procure input data. Some mitigations:  Publish legal disclaimers and explanations about the context and weakness of the model.  Ensure that the models or algorithms are well-documented.  Release models or algorithms via limited access by requiring user registration.  Train the model on a limited data set, to deliberately make the open-source model weaker.  That would still be useful for testing or learning purposes.  Ensure that sensitive information is scrubbed from the model. i. Accidents This refers to the accidental release of any data, whether anonymised, whether PII or otherwise, or any data that a company needs to retain to maintain its competitive advantage. Some mitigations:  Include legal disclaimers regarding potential data threats.  Ensure data is anonymised before being available for third-party or internal analysis. --- Page 12 --- Page 12 of 18 j. Environment This refers to training data of AI models which are valid in specific contexts or environments. Any change to the context could result in these models giving erroneous outcomes. Some mitigations:  Track false positive and false negative rates of the model.  Track the model's standard output/outcome to see if it has changed over time.  Periodically audit the model to validate its performance.  Keep a tab on major global events that may need a change in the model's algorithm. k. Explainability & Interpretability of Models While many AI systems predict, classify or optimise well, they may only offer an answer in associated probabilities. It does not explain how it arrived at the answer. This lack of explainability could frustrate users, especially those for whom the system's output is unfavourable. Some mitigations:  SHAP The "Shapley Additive explanation" is a recent technique that applies a game theory approach to assess how each input variable affects the final result of the trained model[19]. While the SHAP tool works across any ML model, it has an exacting approach for tree ensemble methods – as well as good support for deep neural network models, build on Keras/Tensorflow. These explainer tools are available at https://github.com/slundberg/shap  Lower complexity model The data science team may attempt to train models of lower complexity like polynomial regressions/multi-class logistic classification/decision trees/random forest to see if these estimators form a sound basis for the results. These simpler models may allow for the identification of a few dominant variables that can provide a partial explanation of how the variables and results are related.  PCA Principal Component Analysis ("PCA") for dimension reduction. Companies should research best practices for data analytics in their industry or ML domain to identify the engineered features that are used in the domain. These features are typically identified based on domain knowledge and operational experience; they may carry higher signal-to-noise ratios versus generic data points. They may also provide more intuitive results for domain experts. PCA is used to reduce the number of inputs, which in turn reduces the complexity of the problem.  Stacking of ML models Different ML methods can be stacked together serially, where each down-stream model performs a more refined classification on different subsets of the problem. You can decompose the problem first by unsupervised cluster methods or supervised algorithms (for example, identify the type of animal, a "dog"). Next, add another layer of classification (for example, identify the sub-breed, a "Husky"). You can then gain insights into each layered step of the model and improve the composite model's explainability. A similar thought process can be applied to regression problems for increased explainability of dominant factors of each substate. Under certain conditions, stacking models offer more explainability of the models being deployed [20]–[22]. However, while models can be stacked ad-infinitum, performance gains may plateau after a couple of models; longer chains may result in significant information loss For long chains, it may be tough to evaluate how the explainability of constituent models can relate to the explainability of the composite model[20]. --- Page 13 --- Page 13 of 18  Intuitive explanations to non-technical users While AI systems may be black boxes, the empirical processes and statistical models may be explained to the end-user using intuitive real-world analogues. It is essential to discuss the context which the model applies, its constraints, how to interpret the results and the potential failure states of the model. One example: "counterfactual explanations" or statements of how the world would have to be different for the desired outcome to occur. For instance: "You were denied a loan because your annual income was $30,
000. If your income had been $45,000, you would have been offered a loan"[23].  Communication and education It is essential to set up clear communication channels for internal users or end-users to communicate to the project team about any concerns or problems they may encounter. This would allow the operating team to better understand and empathise with the perspectives of end-users, and educate users about ML models and their applications.
3.2 Stress Testing Stress-testing is an effective way to reveal vulnerabilities in AI systems that might not be immediately obvious. Here are some strategies for stress-testing AI systems: a. Rollbacks A well-designed IT system is typically easy to rollback and reset. This will support rapid, iterative software development. Organisations should test if the AI system can be easily reset and retrained to an earlier model. Organisations should also monitor lag time, performance deficits and data integrity. b. Noise Incomplete data, or data with noise/interference may skew the outcome of an AI program. A robust AI system should be programmed to know how to handle incomplete data and data with noise. This allows the system to be more robust against data poisoning attacks, and ensure that the AI system is not overfitted on training data. c. Load-testing Conducting load-testing on a range of use-cases would be ideal. This is a definitive test to see how the system performs at scale. Ensure that the tests are varied and conducted on the most resource-intensive aspects of the system, such as exception loops when human decisions are involved. d. Random failures Shut-down sub-component and critical resources at random. This will test how the system deals with random failures of subsystems. It is also to test how the system would respond in the event of a critical failure. e. Alerts Testing of monitoring and alerting tools of the AI system should be regularly done. This is to obtain valid, timely and granular information to monitor loading. Validate rules that are in place to alert operators of any errors or malfunctions. --- Page 14 --- Page 14 of 18
3.3 Iterative Design & Testing The development of AI systems follows an iterative design process in which testing and improvements are made at various stages of development. This offers opportunities to test out initial ideas in a safe way and mitigate potential failures. At various stages of development, the emphases of testing may vary. As an organisation iterates through these stages, focus on what tests to conduct, and what to look out for. Here is a suggested testing template: Concept- ualisation Algo Selection Model Training DevOps/Beta Production Testing should be on systems architecture, business proposals, empirical observations & assumptions Testing should be on performance of algorithms, errors, edge cases, etc. Model Sandbox: Anonymised and randomly sampled subset of the production data, which can be destructively used to train the data Testing should be on noisy real-world data. AI systems can be tested with little impact on the real world. Testing should be on continuous validity and ensuring systems are well-behaved, even at scale.
3.4 Testing Environments Testing environments are meant to replicate real-world scenarios virtually. Yet, the nature of the risk is different. Testing of AI in simulated environments requires a different and adaptive approach in constructing sandboxes. Sandboxes offer a safe space for the participants, usually data scientists, programmers and solution architects. Others may include the regulating authority, the participating business entities, and civil society. Every sandbox has its own rules depending on what is being tested and the risk appetite of the relevant stakeholders. There are two kinds of sandboxes: a. AI Sandbox An AI sandbox is set up to test hardware, software, data, tools, interfaces, and policies. It is an isolated environment meant for testing and/or preventing malicious programs from inflicting damage. A sandbox can also be set up on a dedicated space on a hard disk or a virtual machine. One critical application is to monitor the system to see how it reacts to specific programs. b. Regulatory Sandbox By contrast, a regulatory sandbox is a process and a tool for regulation. It is a "lab environment", but its vital function is to test innovations against the existing regulatory framework. This is achieved with the participating businesses and the government regulator. --- Page 15 --- Page 15 of 18
3.5 Monitoring All user system access and changes to the system should be logged as it is being trained and deployed. That is to disincentivise tampering and to allow for any potential system abuse to be investigated and resolved. Should the AI system malfunction, the logs would help project teams follow the trail of events that led to the anomaly. For AI systems that cannot be explained, detailed logs allow project teams to have some amount of visibility into the system to ensure that it is functioning as expected. Logging should comprise:  Information, alerts and error logs from the AI model.  Verbose output of how the AI model arrives at a specific decision.  Testing and validation data by other pre-production/dev-test components.  Periodic production system testing scripts/test cases.  System events and logs from the underlying infrastructure (GPU, VMs, databases, analytics engine that runs the AI model, and others).  All events related to user interaction with any part of the systems, ideally behind a single-sign-on with unique profiles for users. This approach is similar to the metric-tracking practices adopted in Site Reliability Engineering[24], but with further logging of application and model level insights from the output of the AI system.
4. Checklist The following points may help while deploying risk-management plans for AI systems: a. Documentation:  Is the system well-documented in terms of its code, APIs and user manual/s?  If the system comes with a pre-trained model, are the data science and statistical processes used to generate the base model well-documented and statistically robust?  Are the caveats and limits of the system made explicit in the documentation? b. Battle-Tested in Production:  Has the AI system been put into production by one or more companies (i) at scale, (ii) for a reasonable period, (iii) for a similar use-case?  Has the system been tested with potential end-users?  What are some reported concerns and limitations that have emerged during these tests? c. Expert Validation:  Have there been any independent third-party reviews of the systems by experts and consultants?  Are these third-party reviews generally positive?  What limitations have the experts identified and validated? d. Limitations on Liabilities:  Are contractual obligations with the technology vendor clear and well-defined?  How is liability distributed in case of system failure? --- Page 16 --- Page 16 of 18 e. Costs of Recovery:  Does the system handle failures or exceptions well while informing the operators of the system?  Or does it crash unexpectedly or, without warning, provide erroneous outputs?  Is it easy and cost-effective to undo the decisions made by the system? f. Locally Tested:  Has the system been tested using the data from the organisation's specific context of use?  Has there been any hand tweaking or selection of data by the vendors or project team to "make the system work"?  Are all the errors and level of uncertainties for each outcome reported clearly by the system? g. If you're using the cloud, here's a checklist for IaaS:  Does your staff have the necessary information on how the cloud works and on cloud hygiene?  Are you deploying data encryption and/or anonymisation?  Have you checked whether your CSP complies with industry standards?  Some standards, depending on your needs: PDPA[17], GDPR[25], HIPAA[26], PCI-DSS[27], SOC2[28].  Can your team manage data in the cloud, instead of downloading to local devices?  Do you monitor shadow IT usage, which is only limited to specific staff or teams?  Are you using the public cloud's native IAM and security group to enforce access control?  Does your team know IT governance policies and which policies cannot be supported?  Some of those policies may include: write on delete, process-driven request for datasets, etc.  Does your team ensure that all cloud API calls and interactions are monitored? h. If you're using the cloud, here's a checklist for PaaS & SaaS:  Does your team understand the limitations and context for the AI APIs provided?  Does your team know that AI errors are the responsibility of your organisation?  That AI errors can't be blamed on computing resources and uptime-related SLAs?  Has your team run all necessary statistical analysis to validate the performance and predictive capabilities of the APIs provided? i. Third-party software providers:  Does your team use CASB (cloud access security brokers) to enforce security policies?  Does your team know which over aspects of the system does the vendor have control?  For which aspects of the system is the vendor reliant on third-party subsystems?  What are the SLAs/performance guarantees the third-party vendor provides?  To which third party controls and visibility will your team have access?  How will the vendor train your team to manage them? --- Page 17 --- Page 17 of 18
5. References [1] "Confronting AI risks | McKinsey." https://www.mckinsey.com/business- functions/mckinsey-analytics/our-insights/confronting-the-risks-of-artificial- intelligence (accessed Jun. 07, 2020). [2] Sebastian, SIEBT, "From the A bomb to the AI bomb, nuclear weapons' problematic evolution," France 24,
2019. https://www.france
24.com/en/20190510-nuclear- weapons-artificial-intelligence-ai-missiles-bombs-technology-military (accessed Jun. 07, 2020). [3] Digital/McKinsey, "Driving impact at scale from automation and AI,"
2019. https://www.mckinsey.com/~/media/McKinsey/Business%20Functions/McKinsey%2 0Digital/Our%20Insights/Driving%20impact%20at%20scale%20from%20automation %20and%20AI/Driving-impact-at-scale-from-automation-and-AI.ashx (accessed Aug. 23, 2020). [4] B. Goggin, "Inside Facebook's suicide algorithm: Here's how the company uses artificial intelligence to predict your mental state from your posts,"
2019. https://www.businessinsider.com/facebook-is-using-ai-to-try-to-predict-if-youre- suicidal-2018-12 (accessed Jun. 07, 2020). [5] Haohui, "Adversarial Attacks in Machine Learning and How to Defend Against Them," Medium,
2019. https://towardsdatascience.com/adversarial-attacks-in-machine- learning-and-how-to-defend-against-them-a2beed95f49c (accessed Jun. 07, 2020). [6] K. Ren, T. Zheng, Z. Qin, and X. Liu, "Adversarial Attacks and Defenses in Deep Learning," Engineering, vol. 6, no. 3, pp. 346–360, Mar. 2020, doi:
10.1016/j.eng.
2019.
12.
012. [7] K. Warr, Strengthening Deep Neural Networks: Making AI Less Susceptible to Adversarial Trickery. O'Reilly Media, Inc.,
2019. [8] D. M. Bourg and G. Seemann, AI for Game Developers. O'Reilly Media, Inc.,
2004. [9] M. A. Nielsen, "Neural Networks and Deep Learning," 2015, Accessed: Aug. 24,
2020. [Online]. Available: http://neuralnetworksanddeeplearning.com. [10] "Simulated Annealing - an overview," ScienceDirect Topics. https://www.sciencedirect.com/topics/materials-science/simulated-annealing (accessed Aug. 24, 2020). [11] "Genetic Algorithm - an overview," ScienceDirect Topics. https://www.sciencedirect.com/topics/engineering/genetic-algorithm (accessed Aug. 24, 2020). [12] "Markov Process - an overview," ScienceDirect Topics. https://www-sciencedirect- com.libproxy
1.nus.edu.sg/topics/mathematics/markov-process (accessed Aug. 24, 2020). [13] B. Cheatham, K. Javanmardian, and H. Samandari, "Confronting the risks of artificial intelligence,"
2019. https://www.mckinsey.com/business-functions/mckinsey- analytics/our-insights/confronting-the-risks-of-artificial-intelligence (accessed Jun. 07, 2020). [14] J. J. Bryson and A. Winfield, "Standardising Ethical Design for Artificial Intelligence and Autonomous Systems," Computer, vol. 50, no. 5, pp. 116–119, May 2017, doi:
10.1109/MC.
2017.
154. [15] A. Jobin, M. Ienca, and E. Vayena, "The global landscape of AI ethics guidelines," Nature Machine Intelligence, vol. 1, no. 9, Art. no. 9, Sep. 2019, doi:
10.1038/s42256- 019-0088-
2. [16] J. Fjeld, N. Achten, H. Hilligoss, A. Nagy, and M. Srikumar, "Principled Artificial Intelligence: Mapping Consensus in Ethical and Rights-Based Approaches to Principles for AI," Social Science Research Network, Rochester, NY, SSRN Scholarly Paper ID 3518482, Jan.
2020. doi:
10.2139/ssrn.
3518482. [17] Republic of Singapore, "Personal Data Protection Act, Singapore,"
2012. Accessed: Jun. 07,
2020. [Online]. Available: https://sso.agc.gov.sg/Act/PDPA
2012. --- Page 18 --- Page 18 of 18 [18] M. Brundage et al., "The Malicious Use of Artificial Intelligence: Forecasting, Prevention, and Mitigation," arXiv:
1802.07228 [cs], Feb. 2018, Accessed: Jun. 09,
2020. [Online]. Available: http://arxiv.org/abs/
1802.
07228. [19] S. Lundberg and S.-I. Lee, "A Unified Approach to Interpreting Model Predictions," arXiv:
1705.07874 [cs, stat], Nov. 2017, Accessed: Aug. 26,
2020. [Online]. Available: http://arxiv.org/abs/
1705.
07874. [20] N. F. Rajani and R. J. Mooney, "Ensembling Visual Explanations," in Explainable and Interpretable Models in Computer Vision and Machine Learning, H. J. Escalante, S. Escalera, I. Guyon, X. Baró, Y. Güçlütürk, U. Güçlü, and M. van Gerven, Eds. Cham: Springer International Publishing, 2018, pp. 155–
172. [21] N. F. Rajani and R. J. Mooney, "Stackingwith auxiliary features for visual question answering," 2018, vol. 1, pp. 2217–
2226. [22] A. Barredo Arrieta et al., "Explainable Artificial Intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI," Information Fusion, vol. 58, pp. 82–115, Jun. 2020, doi:
10.1016/j.inffus.
2019.
12.
012. [23] S. Wachter, B. Mittelstadt, and C. Russell, "Counterfactual Explanations without Opening the Black Box: Automated Decisions and the GDPR," arXiv:
1711.00399 [cs], Mar. 2018, Accessed: Aug. 26,
2020. [Online]. Available: http://arxiv.org/abs/
1711.
00399. [24] B. Beyer, C. Jones, J. Petoff, and N. R. Murphy, Site Reliability Engineering: How Google Runs Production Systems. O'Reilly Media, Inc.,
2018. [25] "General Data Protection Regulation, European Union,"
2016. https://gdpr-info.eu/ (accessed Jun. 07, 2020). [26] "Health Insurance Portability and Accountability Act of 1996,"
1996. https://www.cdc.gov/phlp/publications/topic/hipaa.html (accessed Jun. 07, 2020). [27] "Payment Card Industry (PCI), Data Security Standard, Version
3.
2.1,"
2018. https://www.pcisecuritystandards.org/pci_security/ (accessed Jun. 07, 2020). [28] AICPA, "SOC 2 - SOC for Service Organizations: Trust Services Criteria." https://www.aicpa.org/interestareas/frc/assuranceadvisoryservices/aicpasoc2report.ht ml (accessed Jun. 07, 2020).