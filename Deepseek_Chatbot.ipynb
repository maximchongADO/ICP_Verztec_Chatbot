{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76222841",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "def load_cleaned_texts(folder_path):\n",
    "    texts, metadatas = [], []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            with open(os.path.join(folder_path, filename), \"r\", encoding=\"utf-8\") as f:\n",
    "                raw_text = f.read()\n",
    "            splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "            chunks = splitter.split_text(raw_text)\n",
    "            texts.extend(chunks)\n",
    "            metadatas.extend([{\"source\": filename}] * len(chunks))\n",
    "    return texts, metadatas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bdba2dc",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "texts, metadatas = load_cleaned_texts(\"C:\\\\Users\\\\ethan\\\\OneDrive\\\\Desktop\\\\Bot_reponse_data\\\\cleaned\")\n",
    "\n",
    "vector_store = FAISS.from_texts(texts=texts, embedding=embedding_model, metadatas=metadatas)\n",
    "vector_store.save_local(\"verztec_vector_store\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e0c11e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_community.chat_models import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "import os\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "\n",
    "# ✅ Set OpenRouter API details\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
    "os.environ[\"OPENAI_API_BASE\"] = \"https://openrouter.ai/api/v1\"\n",
    "\n",
    "# ✅ Load embedding model (same as before)\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# ✅ Load FAISS vector store\n",
    "vector_store = FAISS.load_local(\n",
    "    \"verztec_vector_store\",\n",
    "    embedding_model,\n",
    "    allow_dangerous_deserialization=True\n",
    ")\n",
    "\n",
    "# ✅ Create retriever\n",
    "retriever = vector_store.as_retriever(search_type=\"similarity\", k=3)\n",
    "\n",
    "# ✅ Define the prompt template for structured response generation\n",
    "prompt_template = \"\"\"\n",
    "You are a helpful assistant for Verztec employees. Answer the following question based on the provided context from Verztec's internal guidelines.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "# ✅ Apply the prompt template\n",
    "template = PromptTemplate.from_template(prompt_template)\n",
    "\n",
    "# ✅ Load OpenRouter DeepSeek model into LangChain\n",
    "llm = ChatOpenAI(\n",
    "    model_name=\"tngtech/deepseek-r1t-chimera:free\",\n",
    "    temperature=0,\n",
    "    openai_api_key=os.environ[\"OPENAI_API_KEY\"],\n",
    "    openai_api_base=os.environ[\"OPENAI_API_BASE\"]\n",
    ")\n",
    "\n",
    "# ✅ Create LLMChain with the prompt template\n",
    "llm_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=template\n",
    ")\n",
    "\n",
    "# ✅ Build RetrievalQA chain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm_chain,\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True\n",
    ")\n",
    "\n",
    "# ✅ Query and response\n",
    "query = \"what if i have a question during a digital meeting\"\n",
    "\n",
    "# Retrieve documents for context\n",
    "docs = retriever.get_relevant_documents(query)\n",
    "\n",
    "# Prepare the context from the retrieved documents\n",
    "context = \"\\n\".join([doc.page_content for doc in docs])  # Concatenate text from all retrieved docs\n",
    "\n",
    "# Now, invoke the LLM chain using the query and context\n",
    "response = qa_chain.invoke({\n",
    "    \"context\": context,  # Pass the context\n",
    "    \"query\": query # Pass the query\n",
    "\n",
    "})\n",
    "\n",
    "# ✅ Output the response and sources\n",
    "print(\"Answer:\", response[\"result\"])\n",
    "print(\"\\nSources:\")\n",
    "for doc in response[\"source_documents\"]:\n",
    "    print(\"-\", doc.metadata[\"source\"])\n",
    "    \n",
    "    \n",
    "# Error regarding \"Missing some input keys:{'context'}\""
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
